{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mUsing local RoPE.jl (dev env)\n"
     ]
    }
   ],
   "source": [
    "using Flux, DLProteinFormats, Onion, RandomFeatureMaps, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(locs = Float32[-1.3002242 -1.1566908 … 0.62550926 0.71040916; -0.7613289 -0.8056286 … -1.7723281 -1.6092618; 2.3987129 2.1252809 … 2.8664804 3.1106803;;; 0.6027733 0.3164402 … 0.7914734 0.6447735; 0.45070457 0.3482378 … 0.08787127 -0.01456213; -0.32564774 -0.32888108 … 0.71605223 0.964052;;; 1.2801039 1.4215373 … -0.46776256 -0.6848625; -0.5331842 -0.47061747 … -0.06838412 0.11204921; -1.0315678 -1.2856345 … 1.4772322 1.580299;;; 1.9724281 2.2653618 … -1.061238 -0.9377052; -0.05254669 0.03258667 … -0.19698028 -0.10661392; 0.21636124 0.3799612 … -1.4370722 -1.6989721;;; 1.9372044 2.1661048 … -0.20676212 -0.016295433; -0.23857078 -0.43613777 … -0.3800045 -0.5025711; -0.65939635 -0.6507961 … -1.4745295 -1.265763;;; 0.071567915 -0.12039852 … -0.28156528 -0.4357319; 1.063317 1.3041168 … 1.0247504 0.86725044; 3.474465 3.422398 … -0.7119687 -0.9248353;;; 1.4760973 1.4956639 … -0.7350361 -0.88353616; 0.30170518 0.554005 … -0.3175947 -0.53592813; -0.9225462 -1.1101129 … -0.59364617 -0.7488128;;; 0.2824439 0.2190772 … 0.78597707 0.6873439; 0.5666458 0.88687915 … 0.4741125 0.79321253; -0.744034 -0.8761339 … 0.017266084 0.0675663;;; -1.5823307 -1.825231 … -2.2900643 -2.4437308; 0.5620964 0.37249678 … 0.78256303 0.81176376; 0.36725348 0.45565376 … 1.2549534 0.94595337;;; -0.9453535 -0.7419202 … -1.6400535 -1.8604202; 0.9508794 0.72371244 … 0.9799794 0.7781127; -0.4145422 -0.31514207 … -0.94417554 -0.8622422], AAs = Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 1 … 0 1; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 1; … ; 0 0 … 0 0; 0 0 … 1 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = DLProteinFormats.load(PDBSimpleFlat500);\n",
    "\n",
    "L = 30\n",
    "train_inds = findall(dat.len .> L)\n",
    " \n",
    "function random_batch(dat, L, B, filt_inds)\n",
    "    locs = zeros(Float32, 3, L, B)\n",
    "    inds = sample(filt_inds, B, replace=false)\n",
    "    AAs = zeros(Int, L, B)\n",
    "    for (i,ind) in enumerate(inds)\n",
    "        l_range = rand(1:dat[ind].len - L + 1)\n",
    "        locs[:, :, i] = dat[ind].locs[:, 1, l_range:l_range+L-1]\n",
    "        AAs[:, i] = dat[ind].AAs[l_range:l_range+L-1]\n",
    "    end\n",
    "    return (;locs, AAs = Flux.onehotbatch(AAs, 1:20))\n",
    "end\n",
    "\n",
    "batch = random_batch(dat, L, 10, train_inds);\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Toy1{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy1\n",
    "function Toy1(dim, depth)\n",
    "    layers = (;\n",
    "        #loc_encoder = Dense(3 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8, rope=Onion.MultiDimRoPE(8, 3)) for _ in 1:depth],\n",
    "        #AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy1(layers)\n",
    "end\n",
    "function (m::Toy1)(x)\n",
    "    l = m.layers\n",
    "    for transformerblock in l.transformers\n",
    "        x = transformerblock(x, 0, nothing, x_pos = x_pos)\n",
    "        \n",
    "    end\n",
    "    return x \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float32[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Toy1(64, 1)\n",
    "opt_state = Flux.setup(AdamW(eta = 0.001), model)\n",
    "losses = Float32[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2×1 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       "  1.71674    -0.432757\n",
       " -0.0165048   0.661345\n",
       " -0.34529     0.445952"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x =  randn(Float32, 64, 2, 1)\n",
    "x_pos = randn(Float32, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059496827\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: (\"Output and shifted output not equal, mean differnce = \", nothing)",
     "output_type": "error",
     "traceback": [
      "AssertionError: (\"Output and shifted output not equal, mean differnce = \", nothing)\n",
      "\n",
      "Stacktrace:\n",
      " [1] (::Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}})(x_query::Array{Float32, 3}, x_key::Array{Float32, 3}, start_pos::Int64, mask::Int64; rope::Onion.MultiDimRoPE{Matrix{Float32}}, x_pos::Array{Float32, 3})\n",
      "   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:147\n",
      " [2] Attention\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:91 [inlined]\n",
      " [3] #_#7\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:88 [inlined]\n",
      " [4] Attention\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:87 [inlined]\n",
      " [5] (::TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}, Onion.MultiDimRoPE{Matrix{Float32}}})(x::Array{Float32, 3}, start_pos::Int64, rope::Nothing; x_pos::Array{Float32, 3}, mask::Int64)\n",
      "   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:210\n",
      " [6] (::Toy1{@NamedTuple{transformers::Vector{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}, Onion.MultiDimRoPE{Matrix{Float32}}}}}})(x::Array{Float32, 3})\n",
      "   @ Main c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:17\n",
      " [7] top-level scope\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for epoch in 1:20 # 1:100\n",
    "    tot_loss = 0f0\n",
    "    for i in 1:1_000 # 1:10_000\n",
    "        batch = random_batch(dat, L, 10, train_inds);\n",
    "        l, grad = Flux.withgradient(model) do m\n",
    "            aalogits = m(batch.locs)\n",
    "            Flux.logitcrossentropy(aalogits, batch.AAs)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grad[1])\n",
    "        tot_loss += l\n",
    "        if mod(i, 50) == 0\n",
    "            println(epoch, \" \", i, \" \", tot_loss/50)\n",
    "            push!(losses, tot_loss/50)\n",
    "            tot_loss = 0f0\n",
    "        end\n",
    "        (mod(i, 500) == 0) && savefig(plot(losses), \"losses_toy_MultiDimRoPE.pdf\")\n",
    "    end\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
