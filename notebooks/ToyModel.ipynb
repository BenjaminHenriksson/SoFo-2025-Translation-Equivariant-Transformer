{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "#Pkg.add([\"Flux\", \"DLProteinFormats\", \"Onion\", \"RandomFeatureMaps\", \"StatsBase\", \"Plots\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, DLProteinFormats, Onion, RandomFeatureMaps, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = DLProteinFormats.load(PDBSimpleFlat500);\n",
    "\n",
    "L = 30\n",
    "train_inds = findall(dat.len .> L)\n",
    " \n",
    "function random_batch(dat, L, B, filt_inds)\n",
    "    locs = zeros(Float32, 3, L, B)\n",
    "    inds = sample(filt_inds, B, replace=false)\n",
    "    AAs = zeros(Int, L, B)\n",
    "    for (i,ind) in enumerate(inds)\n",
    "        l_range = rand(1:dat[ind].len - L + 1)\n",
    "        locs[:, :, i] = dat[ind].locs[:, 1, l_range:l_range+L-1]\n",
    "        AAs[:, i] = dat[ind].AAs[l_range:l_range+L-1]\n",
    "    end\n",
    "    return (;locs, AAs = Flux.onehotbatch(AAs, 1:20))\n",
    "end\n",
    "\n",
    "batch = random_batch(dat, L, 10, train_inds);\n",
    " \n",
    "struct Toy0{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy0\n",
    "function Toy0()\n",
    "    layers = (;\n",
    "        AA_decoder = Dense(3 => 20, bias=false),\n",
    "    )\n",
    "    return Toy0(layers)\n",
    "end\n",
    "function (m::Toy0)(locs)\n",
    "    l = m.layers\n",
    "    aa_logits = l.AA_decoder(locs)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    "struct Toy1{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy1\n",
    "function Toy1(dim, depth)\n",
    "    layers = (;\n",
    "        loc_encoder = Dense(3 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8, Onion.MultiDimRoPE(Int(dim/8), 3)) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy1(layers)\n",
    "end\n",
    "function (m::Toy1)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(locs)\n",
    "    for transformerblock in l.transformers\n",
    "        x = transformerblock(x, 0, nothing, locs)\n",
    "        #locs = updatelocs(x, locs)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    "struct Toy2{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy2\n",
    "function Toy2(dim, depth)\n",
    "    layers = (;\n",
    "        loc_rff = RandomFourierFeatures(3 => 64, 0.1f0),\n",
    "        loc_encoder = Dense(64 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy2(layers)\n",
    "end\n",
    "function (m::Toy2)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(l.loc_rff(locs))\n",
    "    for layer in l.transformers\n",
    "        x = layer(x, 0, nothing)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching TransformerBlock(::Int64, ::Int64, ::Onion.MultiDimRoPE{Matrix{Float32}})\nThe type `TransformerBlock` exists, but no method is defined for this combination of argument types when trying to construct it.\n\nClosest candidates are:\n  TransformerBlock(::A, ::F, ::AN, !Matched::FN, !Matched::R) where {A, F, AN, FN, R}\n   @ Onion C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:138\n  TransformerBlock(::Int64, ::Int64; ...)\n   @ Onion C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:170\n  TransformerBlock(::Int64, ::Int64, !Matched::Int64; ...)\n   @ Onion C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:170\n  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching TransformerBlock(::Int64, ::Int64, ::Onion.MultiDimRoPE{Matrix{Float32}})\n",
      "The type `TransformerBlock` exists, but no method is defined for this combination of argument types when trying to construct it.\n",
      "\n",
      "Closest candidates are:\n",
      "  TransformerBlock(::A, ::F, ::AN, !Matched::FN, !Matched::R) where {A, F, AN, FN, R}\n",
      "   @ Onion C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:138\n",
      "  TransformerBlock(::Int64, ::Int64; ...)\n",
      "   @ Onion C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:170\n",
      "  TransformerBlock(::Int64, ::Int64, !Matched::Int64; ...)\n",
      "   @ Onion C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:170\n",
      "  ...\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] (::var\"#19#20\"{Int64})(::Int64)\n",
      "   @ Main .\\none:0\n",
      " [2] iterate\n",
      "   @ .\\generator.jl:48 [inlined]\n",
      " [3] collect\n",
      "   @ .\\array.jl:791 [inlined]\n",
      " [4] Toy1(dim::Int64, depth::Int64)\n",
      "   @ Main c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:41\n",
      " [5] top-level scope\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "#model = Toy0()\n",
    "model = Toy1(64, 4)\n",
    "#model = Toy2(64, 4)\n",
    "opt_state = Flux.setup(AdamW(eta = 0.001), model)\n",
    " \n",
    "losses = Float32[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `C:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Project.toml`\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[df93d313] \u001b[39mDLProteinFormats v0.1.2\n",
      "  \u001b[90m[587475ba] \u001b[39mFlux v0.16.4\n",
      "  \u001b[90m[fdebf6c2] \u001b[39mOnion v0.1.1 `Onion.jl`\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.14\n",
      "  \u001b[90m[780baa95] \u001b[39mRandomFeatureMaps v0.2.2\n",
      "  \u001b[90m[2913bbd2] \u001b[39mStatsBase v0.34.5\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m have new versions available and may be upgradable.\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `model` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name may be made accessible by importing BioStructures in the current active module Main",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `model` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name may be made accessible by importing BioStructures in the current active module Main\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:5"
     ]
    }
   ],
   "source": [
    "for epoch in 1:20 # 1:100\n",
    "    tot_loss = 0f0\n",
    "    for i in 1:1_000 # 1:10_000\n",
    "        batch = random_batch(dat, L, 10, train_inds);\n",
    "        l, grad = Flux.withgradient(model) do m\n",
    "            aalogits = m(batch.locs)\n",
    "            Flux.logitcrossentropy(aalogits, batch.AAs)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grad[1])\n",
    "        tot_loss += l\n",
    "        if mod(i, 50) == 0\n",
    "            println(epoch, \" \", i, \" \", tot_loss/50)\n",
    "            push!(losses, tot_loss/50)\n",
    "            tot_loss = 0f0\n",
    "        end\n",
    "        (mod(i, 500) == 0) && savefig(plot(losses), \"losses_toy_MultiDimRoPE.pdf\")\n",
    "    end\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
