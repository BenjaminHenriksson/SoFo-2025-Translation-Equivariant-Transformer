{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "#Pkg.add([\"Flux\", \"DLProteinFormats\", \"Onion\", \"RandomFeatureMaps\", \"StatsBase\", \"Plots\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, DLProteinFormats, Onion, RandomFeatureMaps, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = DLProteinFormats.load(PDBSimpleFlat500);\n",
    "\n",
    "L = 30\n",
    "train_inds = findall(dat.len .> L)\n",
    " \n",
    "function random_batch(dat, L, B, filt_inds)\n",
    "    locs = zeros(Float32, 3, L, B)\n",
    "    inds = sample(filt_inds, B, replace=false)\n",
    "    AAs = zeros(Int, L, B)\n",
    "    for (i,ind) in enumerate(inds)\n",
    "        l_range = rand(1:dat[ind].len - L + 1)\n",
    "        locs[:, :, i] = dat[ind].locs[:, 1, l_range:l_range+L-1]\n",
    "        AAs[:, i] = dat[ind].AAs[l_range:l_range+L-1]\n",
    "    end\n",
    "    return (;locs, AAs = Flux.onehotbatch(AAs, 1:20))\n",
    "end\n",
    "\n",
    "batch = random_batch(dat, L, 10, train_inds);\n",
    " \n",
    "struct Toy0{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy0\n",
    "function Toy0()\n",
    "    layers = (;\n",
    "        AA_decoder = Dense(3 => 20, bias=false),\n",
    "    )\n",
    "    return Toy0(layers)\n",
    "end\n",
    "function (m::Toy0)(locs)\n",
    "    l = m.layers\n",
    "    aa_logits = l.AA_decoder(locs)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    "struct Toy1{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy1\n",
    "function Toy1(dim, depth)\n",
    "    layers = (;\n",
    "        loc_encoder = Dense(3 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8, rope=Onion.MultiDimRoPE) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy1(layers)\n",
    "end\n",
    "function (m::Toy1)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(locs)\n",
    "    for transformerblock in l.transformers\n",
    "        x = transformerblock(x, 0, nothing, locs)\n",
    "        #locs = updatelocs(x, locs)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    "struct Toy2{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy2\n",
    "function Toy2(dim, depth)\n",
    "    layers = (;\n",
    "        loc_rff = RandomFourierFeatures(3 => 64, 0.1f0),\n",
    "        loc_encoder = Dense(64 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy2(layers)\n",
    "end\n",
    "function (m::Toy2)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(l.loc_rff(locs))\n",
    "    for layer in l.transformers\n",
    "        x = layer(x, 0, nothing)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float32[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model = Toy0()\n",
    "model = Toy1(64, 4)\n",
    "#model = Toy2(64, 4)\n",
    "opt_state = Flux.setup(AdamW(eta = 0.001), model)\n",
    " \n",
    "losses = Float32[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  Attention(\n",
       "    Dense(64 => 64; bias=false),        \u001b[90m# 4_096 parameters\u001b[39m\n",
       "    Dense(64 => 64; bias=false),        \u001b[90m# 4_096 parameters\u001b[39m\n",
       "    Dense(64 => 64; bias=false),        \u001b[90m# 4_096 parameters\u001b[39m\n",
       "    Dense(64 => 64; bias=false),        \u001b[90m# 4_096 parameters\u001b[39m\n",
       "    64,\n",
       "    8,\n",
       "    8,\n",
       "    8,\n",
       "  ),\n",
       "  StarGLU(\n",
       "    Dense(64 => 256; bias=false),       \u001b[90m# 16_384 parameters\u001b[39m\n",
       "    Dense(256 => 64; bias=false),       \u001b[90m# 16_384 parameters\u001b[39m\n",
       "    Dense(64 => 256; bias=false),       \u001b[90m# 16_384 parameters\u001b[39m\n",
       "    NNlib.swish,\n",
       "  ),\n",
       "  RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5),  \u001b[90m# 64 parameters\u001b[39m\n",
       "  RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5),  \u001b[90m# 64 parameters\u001b[39m\n",
       "  Onion.MultiDimRoPE,\n",
       ") \u001b[90m                  # Total: 9 arrays, \u001b[39m65_664 parameters, 257.133 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.layers.transformers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 50 3.372465\n",
      "1 100 3.0511887\n",
      "1 150 2.995998\n",
      "1 200 2.98264\n",
      "1 250 2.9905777\n",
      "1 300 2.9700623\n",
      "1 350 2.9567733\n",
      "1 400 2.945958\n",
      "1 450 2.956957\n",
      "1 500 2.9402626\n",
      "1 550 2.934617\n",
      "1 600 2.9285443\n",
      "1 650 2.9337845\n",
      "1 700 2.9316099\n",
      "1 750 2.9213576\n",
      "1 800 2.9174562\n",
      "1 850 2.921789\n",
      "1 900 2.9093676\n",
      "1 950 2.915376\n",
      "1 1000 2.9206727\n",
      "2 50 2.9170485\n",
      "2 100 2.899255\n",
      "2 150 2.9003658\n",
      "2 200 2.9075286\n",
      "2 250 2.8878295\n",
      "2 300 2.9025567\n",
      "2 350 2.8923025\n",
      "2 400 2.904101\n",
      "2 450 2.8907034\n",
      "2 500 2.8906848\n",
      "2 550 2.8873181\n",
      "2 600 2.8805869\n",
      "2 650 2.8794146\n",
      "2 700 2.8790338\n",
      "2 750 2.8782744\n",
      "2 800 2.878731\n",
      "2 850 2.8831854\n",
      "2 900 2.8802595\n",
      "2 950 2.8888102\n",
      "2 1000 2.8699756\n",
      "3 50 2.8699083\n",
      "3 100 2.8646445\n",
      "3 150 2.8553307\n",
      "3 200 2.863233\n",
      "3 250 2.8625739\n",
      "3 300 2.8719995\n",
      "3 350 2.861191\n",
      "3 400 2.8535023\n",
      "3 450 2.851653\n",
      "3 500 2.864762\n",
      "3 550 2.8573709\n",
      "3 600 2.845413\n",
      "3 650 2.845547\n",
      "3 700 2.8695822\n",
      "3 750 2.8478754\n",
      "3 800 2.851848\n",
      "3 850 2.8292806\n",
      "3 900 2.8435218\n",
      "3 950 2.8360178\n",
      "3 1000 2.8423874\n",
      "4 50 2.8462903\n",
      "4 100 2.8544028\n",
      "4 150 2.8419824\n",
      "4 200 2.848052\n",
      "4 250 2.8509214\n",
      "4 300 2.84797\n",
      "4 350 2.8299334\n",
      "4 400 2.8463311\n",
      "4 450 2.8263865\n",
      "4 500 2.8222897\n",
      "4 550 2.8331\n",
      "4 600 2.8044884\n",
      "4 650 2.8182862\n",
      "4 700 2.8315623\n",
      "4 750 2.822085\n",
      "4 800 2.8083923\n",
      "4 850 2.836122\n",
      "4 900 2.818306\n",
      "4 950 2.8241618\n",
      "4 1000 2.8251104\n",
      "5 50 2.8195858\n",
      "5 100 2.8184557\n",
      "5 150 2.8189936\n",
      "5 200 2.791939\n",
      "5 250 2.819855\n",
      "5 300 2.8179274\n",
      "5 350 2.8123095\n",
      "5 400 2.7909808\n",
      "5 450 2.804511\n",
      "5 500 2.796283\n",
      "5 550 2.787783\n",
      "5 600 2.805638\n",
      "5 650 2.7980514\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1:20 # 1:100\n",
    "    tot_loss = 0f0\n",
    "    for i in 1:1_000 # 1:10_000\n",
    "        batch = random_batch(dat, L, 10, train_inds);\n",
    "        l, grad = Flux.withgradient(model) do m\n",
    "            aalogits = m(batch.locs)\n",
    "            Flux.logitcrossentropy(aalogits, batch.AAs)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grad[1])\n",
    "        tot_loss += l\n",
    "        if mod(i, 50) == 0\n",
    "            println(epoch, \" \", i, \" \", tot_loss/50)\n",
    "            push!(losses, tot_loss/50)\n",
    "            tot_loss = 0f0\n",
    "        end\n",
    "        (mod(i, 500) == 0) && savefig(plot(losses), \"losses_toy_MultiDimRoPE.pdf\")\n",
    "    end\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
