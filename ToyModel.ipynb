{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "#Pkg.add([\"Flux\", \"DLProteinFormats\", \"Onion\", \"RandomFeatureMaps\", \"StatsBase\", \"Plots\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, DLProteinFormats, Onion, RandomFeatureMaps, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 50 3.4911306\n",
      "1 100 3.0472863\n",
      "1 150 3.01849\n",
      "1 200 2.9722974\n",
      "1 250 2.9606898\n",
      "1 300 2.958691\n",
      "1 350 2.9460843\n",
      "1 400 2.939723\n",
      "1 450 2.930393\n",
      "1 500 2.931485\n",
      "1 550 2.9275763\n",
      "1 600 2.918871\n",
      "1 650 2.909648\n",
      "1 700 2.911985\n",
      "1 750 2.901052\n",
      "1 800 2.9014356\n",
      "1 850 2.9049335\n",
      "1 900 2.8998847\n",
      "1 950 2.8961618\n",
      "1 1000 2.899681\n",
      "2 50 2.8922176\n",
      "2 100 2.8984327\n",
      "2 150 2.889498\n",
      "2 200 2.898843\n",
      "2 250 2.8897498\n",
      "2 300 2.8883402\n",
      "2 350 2.88637\n",
      "2 400 2.8824177\n",
      "2 450 2.876782\n",
      "2 500 2.883402\n",
      "2 550 2.8808005\n",
      "2 600 2.8742821\n",
      "2 650 2.8735802\n",
      "2 700 2.870503\n",
      "2 750 2.867412\n",
      "2 800 2.8741212\n",
      "2 850 2.87578\n",
      "2 900 2.8682227\n",
      "2 950 2.8738542\n",
      "2 1000 2.8694134\n",
      "3 50 2.8631284\n",
      "3 100 2.8688242\n",
      "3 150 2.8584378\n",
      "3 200 2.8599274\n",
      "3 250 2.8522646\n",
      "3 300 2.8653042\n",
      "3 350 2.8578463\n",
      "3 400 2.8551874\n",
      "3 450 2.8587842\n",
      "3 500 2.8461316\n",
      "3 550 2.8598113\n",
      "3 600 2.8554175\n",
      "3 650 2.8587558\n",
      "3 700 2.8481321\n",
      "3 750 2.8597116\n",
      "3 800 2.8406458\n",
      "3 850 2.8399584\n",
      "3 900 2.846041\n",
      "3 950 2.8295605\n",
      "3 1000 2.8429916\n",
      "4 50 2.8316183\n",
      "4 100 2.8363495\n",
      "4 150 2.8384233\n",
      "4 200 2.8223884\n",
      "4 250 2.8130987\n",
      "4 300 2.8170583\n",
      "4 350 2.8290992\n",
      "4 400 2.8244162\n",
      "4 450 2.8273275\n",
      "4 500 2.811522\n",
      "4 550 2.8293848\n",
      "4 600 2.819435\n",
      "4 650 2.8219895\n",
      "4 700 2.8149064\n",
      "4 750 2.8238854\n",
      "4 800 2.811841\n",
      "4 850 2.814841\n",
      "4 900 2.8080454\n",
      "4 950 2.8209238\n",
      "4 1000 2.8176186\n",
      "5 50 2.8143427\n",
      "5 100 2.8138306\n",
      "5 150 2.8110507\n",
      "5 200 2.8167486\n",
      "5 250 2.7861593\n",
      "5 300 2.8072956\n",
      "5 350 2.8019848\n",
      "5 400 2.798596\n",
      "5 450 2.7726777\n",
      "5 500 2.817316\n",
      "5 550 2.795056\n",
      "5 600 2.772713\n",
      "5 650 2.8004162\n",
      "5 700 2.766556\n",
      "5 750 2.7830966\n",
      "5 800 2.8011823\n",
      "5 850 2.7931762\n",
      "5 900 2.7942572\n",
      "5 950 2.7836072\n",
      "5 1000 2.7901504\n",
      "6 50 2.7751014\n",
      "6 100 2.771881\n",
      "6 150 2.7688704\n",
      "6 200 2.7818763\n",
      "6 250 2.7623332\n",
      "6 300 2.7471914\n",
      "6 350 2.755574\n",
      "6 400 2.7543786\n",
      "6 450 2.724581\n",
      "6 500 2.780849\n",
      "6 550 2.7510967\n",
      "6 600 2.7121568\n",
      "6 650 2.7570078\n",
      "6 700 2.733419\n",
      "6 750 2.7747736\n",
      "6 800 2.731801\n",
      "6 850 2.7167017\n",
      "6 900 2.7414134\n",
      "6 950 2.7331207\n",
      "6 1000 2.7313747\n",
      "7 50 2.7523162\n",
      "7 100 2.7291114\n",
      "7 150 2.7109976\n",
      "7 200 2.7356892\n",
      "7 250 2.7476664\n",
      "7 300 2.7246094\n",
      "7 350 2.7285798\n",
      "7 400 2.721167\n",
      "7 450 2.7509632\n",
      "7 500 2.6818585\n",
      "7 550 2.7121148\n",
      "7 600 2.7341743\n",
      "7 650 2.6905758\n",
      "7 700 2.7090588\n",
      "7 750 2.7105136\n",
      "7 800 2.708907\n",
      "7 850 2.6702945\n",
      "7 900 2.6963265\n",
      "7 950 2.6921296\n",
      "7 1000 2.7278833\n",
      "8 50 2.668733\n",
      "8 100 2.6924496\n",
      "8 150 2.7113364\n",
      "8 200 2.6932116\n",
      "8 250 2.6792173\n",
      "8 300 2.6944013\n",
      "8 350 2.680795\n",
      "8 400 2.6825771\n",
      "8 450 2.6886795\n",
      "8 500 2.7030196\n",
      "8 550 2.682565\n",
      "8 600 2.6744812\n",
      "8 650 2.6762881\n",
      "8 700 2.6809647\n",
      "8 750 2.6866102\n",
      "8 800 2.6440115\n",
      "8 850 2.6722944\n",
      "8 900 2.635616\n",
      "8 950 2.6805155\n",
      "8 1000 2.6481998\n",
      "9 50 2.6778982\n",
      "9 100 2.6534986\n",
      "9 150 2.6423361\n",
      "9 200 2.6948762\n",
      "9 250 2.639987\n",
      "9 300 2.6619961\n",
      "9 350 2.6460035\n",
      "9 400 2.6414156\n",
      "9 450 2.6412916\n",
      "9 500 2.662749\n",
      "9 550 2.611377\n",
      "9 600 2.606376\n",
      "9 650 2.6994724\n",
      "9 700 2.5877283\n",
      "9 750 2.6555402\n",
      "9 800 2.6584327\n",
      "9 850 2.5934641\n",
      "9 900 2.6646044\n",
      "9 950 2.6109076\n",
      "9 1000 2.6066232\n",
      "10 50 2.6262782\n",
      "10 100 2.6236656\n",
      "10 150 2.5937786\n",
      "10 200 2.658334\n",
      "10 250 2.6406665\n",
      "10 300 2.6428175\n",
      "10 350 2.591666\n",
      "10 400 2.5760443\n",
      "10 450 2.5837195\n",
      "10 500 2.6495297\n",
      "10 550 2.5844028\n",
      "10 600 2.6282032\n",
      "10 650 2.5762918\n",
      "10 700 2.566731\n",
      "10 750 2.5820847\n",
      "10 800 2.6039152\n",
      "10 850 2.618433\n",
      "10 900 2.6098597\n",
      "10 950 2.6087484\n",
      "10 1000 2.6028054\n",
      "11 50 2.5807471\n",
      "11 100 2.5892675\n",
      "11 150 2.5786722\n",
      "11 200 2.555101\n",
      "11 250 2.6004202\n",
      "11 300 2.57\n",
      "11 350 2.5908325\n",
      "11 400 2.5354917\n",
      "11 450 2.5530539\n",
      "11 500 2.5403543\n",
      "11 550 2.5807695\n",
      "11 600 2.550089\n",
      "11 650 2.5485723\n",
      "11 700 2.6035798\n",
      "11 750 2.5445573\n",
      "11 800 2.5332127\n",
      "11 850 2.5856566\n",
      "11 900 2.5408225\n",
      "11 950 2.5584395\n",
      "11 1000 2.5407841\n",
      "12 50 2.4851525\n",
      "12 100 2.5424461\n",
      "12 150 2.602768\n",
      "12 200 2.5532396\n",
      "12 250 2.5633047\n",
      "12 300 2.6001563\n",
      "12 350 2.5723295\n",
      "12 400 2.5525224\n",
      "12 450 2.5278704\n",
      "12 500 2.5224068\n",
      "12 550 2.4813244\n",
      "12 600 2.5934842\n",
      "12 650 2.5628245\n",
      "12 700 2.513864\n",
      "12 750 2.6178722\n",
      "12 800 2.5133283\n",
      "12 850 2.5683255\n",
      "12 900 2.5843208\n",
      "12 950 2.5494578\n",
      "12 1000 2.4731472\n",
      "13 50 2.564014\n",
      "13 100 2.5438993\n",
      "13 150 2.5207644\n",
      "13 200 2.5114417\n",
      "13 250 2.5105815\n",
      "13 300 2.5091183\n",
      "13 350 2.4987817\n",
      "13 400 2.4931664\n",
      "13 450 2.5294504\n",
      "13 500 2.5239964\n",
      "13 550 2.5354743\n",
      "13 600 2.5680346\n",
      "13 650 2.533094\n",
      "13 700 2.5076761\n",
      "13 750 2.5258765\n",
      "13 800 2.5225022\n",
      "13 850 2.4694893\n",
      "13 900 2.460491\n",
      "13 950 2.5091877\n",
      "13 1000 2.5181296\n",
      "14 50 2.5368316\n",
      "14 100 2.4675503\n",
      "14 150 2.4764564\n",
      "14 200 2.4261982\n",
      "14 250 2.5190513\n",
      "14 300 2.476366\n",
      "14 350 2.4983354\n",
      "14 400 2.501912\n",
      "14 450 2.510411\n",
      "14 500 2.49659\n",
      "14 550 2.4739568\n",
      "14 600 2.4537406\n",
      "14 650 2.4999092\n",
      "14 700 2.5359924\n",
      "14 750 2.4821868\n",
      "14 800 2.521072\n",
      "14 850 2.4975717\n",
      "14 900 2.4655483\n",
      "14 950 2.5011587\n",
      "14 1000 2.5004346\n",
      "15 50 2.4887266\n",
      "15 100 2.4584904\n",
      "15 150 2.4241877\n",
      "15 200 2.4851289\n",
      "15 250 2.505658\n",
      "15 300 2.4685397\n",
      "15 350 2.5031705\n",
      "15 400 2.4711814\n",
      "15 450 2.4883327\n",
      "15 500 2.5096893\n",
      "15 550 2.476372\n",
      "15 600 2.48307\n",
      "15 650 2.463837\n",
      "15 700 2.4324527\n",
      "15 750 2.469782\n",
      "15 800 2.4524071\n",
      "15 850 2.497261\n",
      "15 900 2.4977973\n",
      "15 950 2.460023\n",
      "15 1000 2.4192424\n",
      "16 50 2.4681153\n",
      "16 100 2.4479551\n",
      "16 150 2.4810846\n",
      "16 200 2.485057\n",
      "16 250 2.4928198\n",
      "16 300 2.4964297\n",
      "16 350 2.4741669\n",
      "16 400 2.4572515\n",
      "16 450 2.3745077\n",
      "16 500 2.477646\n",
      "16 550 2.4207299\n",
      "16 600 2.490342\n",
      "16 650 2.4211953\n",
      "16 700 2.454984\n",
      "16 750 2.4573298\n",
      "16 800 2.4393864\n",
      "16 850 2.4816644\n",
      "16 900 2.3938947\n",
      "16 950 2.405113\n",
      "16 1000 2.4689732\n",
      "17 50 2.4228432\n",
      "17 100 2.4219515\n",
      "17 150 2.4244902\n",
      "17 200 2.418943\n",
      "17 250 2.4063292\n",
      "17 300 2.4078646\n",
      "17 350 2.4773223\n",
      "17 400 2.3769565\n",
      "17 450 2.4806838\n",
      "17 500 2.4514976\n",
      "17 550 2.4259896\n",
      "17 600 2.420266\n",
      "17 650 2.378722\n",
      "17 700 2.4356172\n",
      "17 750 2.4910326\n",
      "17 800 2.381731\n",
      "17 850 2.4706874\n",
      "17 900 2.4303117\n",
      "17 950 2.4489872\n",
      "17 1000 2.4209113\n",
      "18 50 2.4221778\n",
      "18 100 2.4251266\n",
      "18 150 2.4397118\n",
      "18 200 2.4862616\n",
      "18 250 2.4072976\n",
      "18 300 2.365919\n",
      "18 350 2.3841014\n",
      "18 400 2.384348\n",
      "18 450 2.4110904\n",
      "18 500 2.4298863\n",
      "18 550 2.4501376\n",
      "18 600 2.3671708\n",
      "18 650 2.4370039\n",
      "18 700 2.3956065\n",
      "18 750 2.4429862\n",
      "18 800 2.3904624\n",
      "18 850 2.4415944\n",
      "18 900 2.3580778\n",
      "18 950 2.396194\n",
      "18 1000 2.3784008\n",
      "19 50 2.389701\n",
      "19 100 2.3544772\n",
      "19 150 2.4058542\n",
      "19 200 2.3643339\n",
      "19 250 2.376979\n",
      "19 300 2.3446662\n",
      "19 350 2.3804917\n",
      "19 400 2.4129746\n",
      "19 450 2.3549948\n",
      "19 500 2.378216\n",
      "19 550 2.4452605\n",
      "19 600 2.350405\n",
      "19 650 2.37152\n",
      "19 700 2.432128\n",
      "19 750 2.345925\n",
      "19 800 2.4285297\n",
      "19 850 2.3788028\n",
      "19 900 2.3923054\n",
      "19 950 2.4447377\n",
      "19 1000 2.4205127\n",
      "20 50 2.399663\n",
      "20 100 2.4583387\n",
      "20 150 2.4429083\n",
      "20 200 2.3639433\n",
      "20 250 2.4623232\n",
      "20 300 2.373337\n",
      "20 350 2.3574564\n",
      "20 400 2.416338\n",
      "20 450 2.3899152\n",
      "20 500 2.4408011\n",
      "20 550 2.3140588\n",
      "20 600 2.3467858\n",
      "20 650 2.4007409\n",
      "20 700 2.4049208\n",
      "20 750 2.4204638\n",
      "20 800 2.3922753\n",
      "20 850 2.4197235\n",
      "20 900 2.343703\n",
      "20 950 2.4039016\n",
      "20 1000 2.4187331\n"
     ]
    }
   ],
   "source": [
    "dat = DLProteinFormats.load(PDBSimpleFlat500);\n",
    "\n",
    "L = 30\n",
    "train_inds = findall(dat.len .> L)\n",
    " \n",
    "function random_batch(dat, L, B, filt_inds)\n",
    "    locs = zeros(Float32, 3, L, B)\n",
    "    inds = sample(filt_inds, B, replace=false)\n",
    "    AAs = zeros(Int, L, B)\n",
    "    for (i,ind) in enumerate(inds)\n",
    "        l_range = rand(1:dat[ind].len - L + 1)\n",
    "        locs[:, :, i] = dat[ind].locs[:, 1, l_range:l_range+L-1]\n",
    "        AAs[:, i] = dat[ind].AAs[l_range:l_range+L-1]\n",
    "    end\n",
    "    return (;locs, AAs = Flux.onehotbatch(AAs, 1:20))\n",
    "end\n",
    "\n",
    "batch = random_batch(dat, L, 10, train_inds);\n",
    " \n",
    "struct Toy0{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy0\n",
    "function Toy0()\n",
    "    layers = (;\n",
    "        AA_decoder = Dense(3 => 20, bias=false),\n",
    "    )\n",
    "    return Toy0(layers)\n",
    "end\n",
    "function (m::Toy0)(locs)\n",
    "    l = m.layers\n",
    "    aa_logits = l.AA_decoder(locs)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    "struct Toy1{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy1\n",
    "function Toy1(dim, depth)\n",
    "    layers = (;\n",
    "        loc_encoder = Dense(3 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy1(layers)\n",
    "end\n",
    "function (m::Toy1)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(locs)\n",
    "    for layer in l.transformers\n",
    "        x = layer(x, 0, nothing)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    "struct Toy2{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy2\n",
    "function Toy2(dim, depth)\n",
    "    layers = (;\n",
    "        loc_rff = RandomFourierFeatures(3 => 64, 0.1f0),\n",
    "        loc_encoder = Dense(64 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy2(layers)\n",
    "end\n",
    "function (m::Toy2)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(l.loc_rff(locs))\n",
    "    for layer in l.transformers\n",
    "        x = layer(x, 0, nothing)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end\n",
    " \n",
    " \n",
    "#model = Toy0()\n",
    "#model = Toy1(64, 4)\n",
    "model = Toy2(64, 4)\n",
    "opt_state = Flux.setup(AdamW(eta = 0.001), model)\n",
    " \n",
    "losses = Float32[]\n",
    "for epoch in 1:20 # 1:100\n",
    "    tot_loss = 0f0\n",
    "    for i in 1:1_000 # 1:10_000\n",
    "        batch = random_batch(dat, L, 10, train_inds);\n",
    "        l, grad = Flux.withgradient(model) do m\n",
    "            aalogits = m(batch.locs)\n",
    "            Flux.logitcrossentropy(aalogits, batch.AAs)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grad[1])\n",
    "        tot_loss += l\n",
    "        if mod(i, 50) == 0\n",
    "            println(epoch, \" \", i, \" \", tot_loss/50)\n",
    "            push!(losses, tot_loss/50)\n",
    "            tot_loss = 0f0\n",
    "        end\n",
    "        (mod(i, 500) == 0) && savefig(plot(losses), \"losses_toy2.pdf\")\n",
    "    end\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
