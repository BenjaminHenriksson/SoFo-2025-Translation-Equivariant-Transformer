{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev env\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n",
    "using Onion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, DLProteinFormats, Onion, RandomFeatureMaps, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(locs = Float32[-3.9385026 -3.6966026 … -3.5770035 -3.7612357; 0.65176547 0.5978653 … 1.1975311 1.1599976; 3.0131392 3.2401397 … 2.994072 3.275505;;; 0.12875375 0.4219207 … -0.34287938 -0.20107937; -2.2870686 -2.3451352 … 0.035398103 -0.1321352; 2.0883386 2.185972 … 3.2084725 3.4243722;;; 0.091526784 0.3406601 … -0.39400655 -0.5211066; 2.3816497 2.26275 … 0.7014165 0.4277831; -3.2392564 -3.1221561 … 1.67681 1.8263104;;; 1.4990135 1.3195808 … 0.5398796 0.57967985; 1.0831373 0.9486038 … 0.47767067 0.74590415; 2.5403047 2.7598388 … 0.8273743 0.9601685;;; -0.5998432 -0.32047692 … 1.5906563 1.664257; -0.8861164 -0.87424964 … -0.017616654 -0.05158348; 0.010995102 0.20426139 … 0.5110611 0.21082763;;; 0.9618603 0.81229365 … 1.0263271 0.9560272; 0.42254257 0.68144286 … 0.8006094 0.6331093; -2.4700665 -2.5526001 … -1.4997667 -1.2546667;;; -1.0587637 -1.3507638 … 0.694703 0.6891364; 1.247051 1.0824511 … 0.7077844 0.8392844; -0.23291817 -0.2066513 … 0.62244856 0.9040815;;; -0.09710542 0.18332791 … 0.1357946 0.054027915; 1.1704476 1.3349808 … -0.82171905 -1.0638192; 1.6058109 1.5513442 … -0.26918918 -0.09685578;;; -0.73917615 -0.5789429 … 1.2780237 0.9638904; -0.061395545 -0.23462887 … -0.03702888 -0.048662208; -0.99338514 -1.2025185 … 0.7732815 0.73181486;;; 1.231541 0.9529747 … -0.097925566 -0.1810257; -5.2009716 -5.113072 … -4.645039 -4.470338; -1.1171147 -1.2896812 … 0.64288557 0.89361894], AAs = Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 1 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 1 0; … ; 0 0 … 0 0; 0 1 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 1 0; … ; 0 0 … 0 0; 0 0 … 0 1;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 1 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = DLProteinFormats.load(PDBSimpleFlat500);\n",
    "\n",
    "L = 30\n",
    "train_inds = findall(dat.len .> L)\n",
    " \n",
    "function random_batch(dat, L, B, filt_inds)\n",
    "    locs = zeros(Float32, 3, L, B)\n",
    "    inds = sample(filt_inds, B, replace=false)\n",
    "    AAs = zeros(Int, L, B)\n",
    "    for (i,ind) in enumerate(inds)\n",
    "        l_range = rand(1:dat[ind].len - L + 1)\n",
    "        locs[:, :, i] = dat[ind].locs[:, 1, l_range:l_range+L-1]\n",
    "        AAs[:, i] = dat[ind].AAs[l_range:l_range+L-1]\n",
    "    end\n",
    "    return (;locs, AAs = Flux.onehotbatch(AAs, 1:20))\n",
    "end\n",
    "\n",
    "batch = random_batch(dat, L, 10, train_inds);\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Toy1{L}\n",
    "    layers::L\n",
    "end\n",
    "Flux.@layer Toy1\n",
    "function Toy1(dim, depth)\n",
    "    layers = (;\n",
    "        loc_encoder = Dense(3 => dim, bias=false),\n",
    "        transformers = [Onion.TransformerBlock(dim, 8, rope=Onion.MultiDimRoPE) for _ in 1:depth],\n",
    "        AA_decoder = Dense(dim => 20, bias=false),\n",
    "    )\n",
    "    return Toy1(layers)\n",
    "end\n",
    "function (m::Toy1)(locs)\n",
    "    l = m.layers\n",
    "    x = l.loc_encoder(locs)\n",
    "    for transformerblock in l.transformers\n",
    "        x = transformerblock(x, 0, nothing, locs)\n",
    "        #locs = updatelocs(x, locs)\n",
    "    end\n",
    "    aa_logits = l.AA_decoder(x)\n",
    "    return aa_logits\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching Attention(::Int64, ::Int64, ::Int64; qkv_bias::Bool, rope::UnionAll)\nThis error has been manually thrown, explicitly, so the method may exist but be intentionally marked as unimplemented.\n\nClosest candidates are:\n  Attention(::Int64, ::Int64, ::Any; qkv_bias) got unsupported keyword argument \"rope\"\n   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:70\n  Attention(::DA, ::DB, ::DC, !Matched::DD, !Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64) where {DA, DB, DC, DD} got unsupported keyword arguments \"qkv_bias\", \"rope\"\n   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:58\n  Attention(::Int64, ::Int64; ...)\n   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:70\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Attention(::Int64, ::Int64, ::Int64; qkv_bias::Bool, rope::UnionAll)\n",
      "This error has been manually thrown, explicitly, so the method may exist but be intentionally marked as unimplemented.\n",
      "\n",
      "Closest candidates are:\n",
      "  Attention(::Int64, ::Int64, ::Any; qkv_bias) got unsupported keyword argument \"rope\"\n",
      "   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:70\n",
      "  Attention(::DA, ::DB, ::DC, !Matched::DD, !Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64) where {DA, DB, DC, DD} got unsupported keyword arguments \"qkv_bias\", \"rope\"\n",
      "   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:58\n",
      "  Attention(::Int64, ::Int64; ...)\n",
      "   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:70\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] kwerr(::@NamedTuple{qkv_bias::Bool, rope::UnionAll}, ::Type, ::Int64, ::Int64, ::Int64)\n",
      "   @ Base .\\error.jl:165\n",
      " [2] TransformerBlock(dim::Int64, n_heads::Int64, n_kv_heads::Int64, ff_hidden_dim::Int64; norm_eps::Float32, qkv_bias::Bool, rope::Type)\n",
      "   @ Onion c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\Onion.jl\\src\\GQAttention.jl:172\n",
      " [3] (::var\"#11#12\"{Int64})(::Int64)\n",
      "   @ Main .\\none:0\n",
      " [4] iterate\n",
      "   @ .\\generator.jl:48 [inlined]\n",
      " [5] collect\n",
      "   @ .\\array.jl:791 [inlined]\n",
      " [6] Toy1(dim::Int64, depth::Int64)\n",
      "   @ Main c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:6\n",
      " [7] top-level scope\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "model = Toy1(64, 4)\n",
    "opt_state = Flux.setup(AdamW(eta = 0.001), model)\n",
    "losses = Float32[]\n",
    "model.layers.transformers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(locs = Float32[-1.0946 -0.8099 … -0.7608333 -0.7133666; 1.9101667 1.7437668 … 2.7523334 2.5680668; -0.7178068 -0.70284003 … 0.10532646 -0.13450699;;; 1.0865543 1.2063221 … -1.0642449 -0.9757118; -0.0429677 -0.17960104 … -0.89093435 -1.207001; 0.58967286 0.8564056 … -0.99806136 -0.97466123;;; -2.2441115 -2.283478 … -1.7469113 -1.6522777; -1.9385716 -1.6220715 … -1.7053382 -1.7297716; -0.26328963 -0.2874565 … -0.119456485 0.20194359;;; -2.192054 -2.120454 … -1.2914875 -1.4886874; 1.5061543 1.2057877 … -0.46401232 -0.41351232; 1.8235438 1.8797104 … 3.2378106 3.4923768;;; 3.348346 3.1180465 … 0.50247955 0.65818024; -0.851297 -0.95419616 … -0.16626282 -0.29263002; -2.33311 -2.50401 … -3.6403441 -3.4063447;;; -1.0151192 -1.061886 … -0.08075257 -0.1506525; -0.20910454 -0.41390437 … -0.8461712 -0.5103712; -0.82430065 -0.59916735 … 1.1374992 1.0903994;;; -0.63200074 -0.7567673 … 0.42483273 0.42863265; 0.33595055 0.15718393 … -0.42574936 -0.70101607; -1.2933768 -1.0786768 … -2.4291434 -2.290977;;; -0.43517208 -0.53053874 … 0.29729456 0.50146127; 0.19891426 -0.11908575 … -0.34271908 -0.3553524; 0.30044422 0.24587753 … -1.2761891 -1.546089;;; -0.30982342 -0.084290124 … 0.9049095 1.1470764; 1.5152452 1.6926119 … 2.5040116 2.7166789; 0.8136668 0.69126683 … 1.5878669 1.5775335;;; 1.4990135 1.3195808 … 0.5398796 0.57967985; 1.0831373 0.9486038 … 0.47767067 0.74590415; 2.5403047 2.7598388 … 0.8273743 0.9601685], AAs = Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 1 … 0 0;;; 1 1 … 0 0; 0 0 … 0 0; … ; 0 0 … 1 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 1 … 0 0; … ; 0 0 … 0 0; 0 0 … 1 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 1 0; … ; 0 0 … 0 0; 0 1 … 0 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random_batch(dat, L, 10, train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `model` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name may be made accessible by importing BioStructures in the current active module Main",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `model` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name may be made accessible by importing BioStructures in the current active module Main\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "model.layers.transformers[1].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `model` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name may be made accessible by importing BioStructures in the current active module Main",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `model` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name may be made accessible by importing BioStructures in the current active module Main\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\User\\Desktop\\SoFo\\code\\SoFo-2025-Translation-Equivariant-Transformer\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X10sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "model(test[1].locs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
